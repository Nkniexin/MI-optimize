rtn:
  kwargs:
    model_type: "llama"
    w_dtype: int4    # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    a_dtype: float16   # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    num_calibrate: 1
    calibrate_seq_length: 2048
    device: 'cuda'
    offload: 'cpu'
    w_groupsize: -1
    a_groupsize: -1
    a_qtype: "per_tensor"              
    w_qtype: "per_channel"             
    w_has_zero: False
    a_has_zero: False
    w_unsign: True
    a_unsign: True
    quantization_type: 'static'
    block_sequential: True
    layer_sequential: True
    skip_layers:                   # the linear layer to skip, should match the module name
      - lm_head
  calibrate_name: 'wikitext2'

gptq:
  kwargs:
    model_type: "llama"
    w_dtype: int4    # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    num_calibrate: 1
    calibrate_seq_length: 2048
    device: 'cuda:7'
    offload: 'cpu'
    blocksize: 128 
    percdamp: .01
    actorder: True
    w_groupsize: -1        
    w_qtype: "per_channel"                    
    block_sequential: True
    layer_sequential: True
    skip_layers:                   # the linear layer to skip, should match the module name
      - lm_head
  calibrate_name: 'wikitext2'

awq:
  kwargs:
    model_type: "llama"
    w_dtype: int4    # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    num_calibrate: 1
    calibrate_seq_length: 2048
    device: 'cuda'
    offload: 'cpu'
    blocksize: 128 
    percdamp: .01
    actorder: True
    w_groupsize: 128        
    w_qtype: "per_group"                    
    block_sequential: False
    layer_sequential: False
    skip_layers:                   # the linear layer to skip, should match the module name
      - lm_head
  calibrate_name: 'wikitext2'

smoothquant:
  kwargs:
    model_type: "llama"
    w_dtype: int4    # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    a_dtype: float16   # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    num_calibrate: 1
    calibrate_seq_length: 2048
    device: 'cuda'
    offload: 'cpu'
    w_groupsize: 128        
    w_qtype: "per_channel" 
    a_qtype: "per_token"                   
    block_sequential: False
    layer_sequential: False
    alpha: 0.5
    quant_out: False 
    skip_layers:                   # the linear layer to skip, should match the module name
      - lm_head
  calibrate_name: 'wikitext2'

zeroquant:
  kwargs:
    model_type: "other_model"
    w_dtype: int4    # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    a_dtype: float16   # select from ['int2', 'int3', 'int4', 'fp4', 'int8', 'fp8', 'float16']
    groupsize: 128
    num_calibrate: 1
    calibrate_seq_length: 2048
    device: 'cuda:7'
    offload: 'cpu'
    w_groupsize: -1
    a_groupsize: -1
    a_qtype: "per_tensor"              
    w_qtype: "per_channel"             
    w_has_zero: False
    a_has_zero: False
    w_unsign: True
    a_unsign: True
    quantization_type: 'static'
    block_sequential: True
    layer_sequential: False
    skip_layers:                   # the linear layer to skip, should match the module name
      - lm_head
  calibrate_name: 'wikitext2'